---
import { Picture } from '@astrojs/image/components'

import ProjectLayout from '../layouts/ProjectLayout.astro'
import Section from '../components/Section.astro'
import SiteLink from '../components/project-links/SiteLink.astro'
import GithubLink from '../components/project-links/GithubLink.astro'
import ListItem from '../components/ListItem.astro'
import Heading from '../components/Heading.astro'

import mainScreenshot from '../images/nemesis-ai/main.png'
import mobileScreenshot from '../images/nemesis-ai/mobile.png'
import debugLog from '../images/nemesis-ai/debug-log.png'
import canvas from '../images/nemesis-ai/canvas.png'
import canvas1 from '../images/nemesis-ai/canvas-1.png'
import canvas2 from '../images/nemesis-ai/canvas-2.png'
import canvas3 from '../images/nemesis-ai/canvas-3.png'
---

<ProjectLayout title='Nemesis AI | Laura Cleveland'>
  <Section>
    <h1 class='mb-16 text-3xl font-bold'>Nemesis AI</h1>
    <p class='mb-10 max-w-xl'>
      Nemesis AI is a bot that I built to answer questions about the rules of
      the board game
      <a
        href='https://awakenrealms.com/games/awaken-realms/nemesis'
        target='_blank'
        rel='noopener noreferrer'
        class='text-[#89d1ed] border-b-2 border-[#77cbec]'
      >
        Nemesis
      </a>.
    </p>
    <p class='mb-12 max-w-xl'>
      The project is a two-part system: the Python scripts that build an index
      from the rules of the game, and the Next app that provides the interface
      and feeds GPT the parts of the index it needs to answer user questions.
    </p>

    <Picture
      src={mainScreenshot}
      alt='screenshot'
      widths={[1024, 768, 400]}
      sizes='(max-width: 1200px) 100vw, 1024px'
      aspectRatio='1297:885'
      class='mb-8 drop-shadow-custom'
    />
  </Section>

  <div
    class='[grid-column:2] w-full h-1 bg-gradient-to-br from-[#67e9d6] to-[#6d6acb]'
  >
  </div>

  <Section>
    <div class='xs:flex xs:flex-wrap'>
      <div class='xs:grow mb-16 xs:mb-0'>
        <h2 class='mb-6 text-xl font-semibold'>Created with</h2>
        <ul>
          <ListItem>Python</ListItem>
          <ListItem>Langchain</ListItem>
          <ListItem>OpenAI API</ListItem>
          <ListItem>React</ListItem>
          <ListItem>Next.js</ListItem>
          <ListItem>Tailwind CSS</ListItem>
        </ul>
      </div>

      <div
        class='shrink-0 flex justify-center xs:flex-col xs:justify-normal gap-4 xs:gap-8 xs:items-end'
      >
        <SiteLink
          url='https://board-game-chatbot-next-7snwbzrxw-laurakcleve.vercel.app/'
        />
        <GithubLink
          url='https://github.com/laurakcleve/board-game-chatbot-next'
        />
      </div>
    </div>
  </Section>

  <Section>
    <Heading>Motivation and purpose</Heading>
    <p class='mb-12 max-w-xl'>
      The emergence of new AI tools has been a great opportunity for me to
      explore different corners of programming and find new things to bring into
      web development.
    </p>
    <p class='max-w-xl'>
      I started trying to create a chatbot with a custom knowledge base by using
      GPT Index, now Llama index. It’s a library that does most of the heavy
      lifting in creating your index and querying it. I tried it out with a
      collection of markdown files that were the content of a blog, but there
      was a lot going on behind the scenes that I didn’t understand. I struggled
      through some projects where I did everything from scratch, which helped me
      learn the mechanisms way better. I refined my techniques over a few
      different iterations of question-answering tools, and after building this
      app, I feel more confident going back and learning the more robust tooling
      out there. I better understand the tradeoffs of different ones, and can
      make more informed decisions about which to use for different situations.
    </p>
  </Section>

  <Section>
    <div class='flex flex-wrap lg:flex-nowrap lg:flex-row'>
      <div class='sm:basis-2/3 md:basis-3/5 lg:basis-2/5'>
        <div
          class='max-w-2xl mb-10 lg:basis-1/2 bg-gradient-to-tr from-[#c77d6b] to-[#dbc188] rounded-xl'
        >
          <div class='py-[7%] px-[27%]'>
            <Picture
              src={mobileScreenshot}
              alt='screenshot'
              widths={[400, 256, 160]}
              sizes='(max-width: 640px) 60vw, (max-width: 1024px) 25vw, 256px'
              aspectRatio='401:867'
              class='drop-shadow-custom'
            />
          </div>
        </div>
      </div>

      <div class='lg:basis-1/2 lg:ml-12'>
        <Heading>Features</Heading>
        <p class='mb-6'>
          The format of the system is a single question and answer, as is pretty
          common for tools like those that let you ask questions about a podcast
          series or YouTube channel. This keeps the UI simple and the text
          generation output more predictable, but with GPT it’s possible to
          implement a continuous conversation as well.
        </p>
      </div>
    </div>

    <p class='max-w-2xl mb-10'>
      One extra thing I added for my own sanity was a debugging log. During
      development I quickly found that debugging LLM inputs and outputs was a
      whole different beast than debugging I had done for any kind of project
      I'd done before. The logs usually consist of large amounts of text that
      are hard to parse at a glance, and the outputs aren’t deterministic. I
      added a little modal that logs out the data for the current answer. It
      shows the parts of the source material that matched the similarity search,
      as well as the sections that were sent to the API as context for the
      answer.
    </p>

    <div
      class='max-w-4xl mb-10 mx-auto lg:basis-1/2 bg-gradient-to-tr from-[#916586] to-[#946666] rounded-xl'
    >
      <div class='p-[8%]'>
        <Picture
          src={debugLog}
          alt='debug log'
          widths={[800, 600, 400]}
          sizes='(max-width: 1024px) 80vw, 800px'
          aspectRatio='800:567'
          class='drop-shadow-custom'
        />
      </div>
    </div>
  </Section>

  <Section>
    <Heading>Challenges</Heading>
    <p class='mb-12 max-w-xl'>
      I ran headlong into the ever-persistent problem of LLMs: accuracy. The
      process of question-answering with a custom knowledge base at its most
      basic is pretty straightforward, but the results can be all over the place
      and it can be extremely difficult to get consistent, accurate answers.
    </p>
    <p class='max-w-xl'>
      I had some difficulty with the common method of retrieval where you use a
      similarity search to retrieve chunks from your data, and feed some number
      of those to the completion model. At first I was having trouble retrieving
      the right information from the source material. I needed to have it split
      into large chunks, to include enough surrounding context, but the
      retrieval method works by selecting chunks based on how similar they are
      to the user’s question. Such large chunks meant that lots of them matched
      the question just as well as others that were more relevant. The context
      passed to the LLM was So I did an extra retrieval step; I made the chunks
      small enough that I would get the most relevant ones, and then I would go
      and get the larger sections that those chunks came from. This helped my
      retrieval accuracy and context quality immensely.
    </p>
  </Section>

  <Section>
    <div class='flex flex-wrap lg:flex-nowrap lg:mb-16'>
      <div class='lg:basis-2/5'>
        <div
          class='mb-10 bg-gradient-to-tr from-[#2c5666] to-[#4e557a] rounded-xl'
        >
          <div class='p-[5%]'>
            <Picture
              src={canvas}
              alt='canvas'
              widths={[600, 410]}
              sizes='(max-width: 640px) 100vw, (max-width: 1024px) 600px, 410px'
              aspectRatio='1200:763'
              class='drop-shadow-custom-small'
            />
          </div>
        </div>
      </div>

      <div class='lg:basis-1/2 lg:ml-12'>
        <Heading>Documenting the process</Heading>
        <p class='mb-12 max-w-xl'>
          I’ve grown more and more fond of keeping notes as I work on a project,
          and on this project I was the most thorough I’ve been thus far. I like
          to use Obsidian for my notes and have been trying out their Canvas
          feature as a new way of laying everything out. These notes will be
          invaluable in putting together a blog post on the process.
        </p>
      </div>
    </div>

    <div
      class='md:inline-block bg-gradient-to-tr from-[#c77d6b] to-[#dbc188] rounded-xl'
    >
      <div
        class='p-[5%] lg:p-[2%] flex flex-col items-center lg:flex-row gap-4 sm:gap-6'
      >
        <div class='max-w-[452px]'>
          <Picture
            src={canvas1}
            alt='screenshot'
            widths={[452, 340]}
            sizes='(max-width: 1200px) 100vw, 1024px'
            aspectRatio='452:227'
            class='drop-shadow-custom-small'
          />
        </div>
        <div class='max-w-[452px]'>
          <Picture
            src={canvas2}
            alt='screenshot'
            widths={[452, 340]}
            sizes='(max-width: 1200px) 100vw, 1024px'
            aspectRatio='452:227'
            class='drop-shadow-custom-small'
          />
        </div>
        <div class='max-w-[452px]'>
          <Picture
            src={canvas3}
            alt='screenshot'
            widths={[452, 340]}
            sizes='(max-width: 1200px) 100vw, 1024px'
            aspectRatio='452:227'
            class='drop-shadow-custom-small'
          />
        </div>
      </div>
    </div>
  </Section>

  <Section>
    <div class='flex justify-center gap-4'>
      <SiteLink
        url='https://board-game-chatbot-next-7snwbzrxw-laurakcleve.vercel.app/'
      />
      <GithubLink
        url='https://github.com/laurakcleve/board-game-chatbot-next'
      />
    </div>
  </Section>
</ProjectLayout>
