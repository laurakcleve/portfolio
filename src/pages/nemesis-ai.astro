---
import { Picture } from '@astrojs/image/components'
import ProjectLayout from '../layouts/ProjectLayout.astro'
import Section from '../components/Section.astro'
import SiteLink from '../components/project-links/SiteLink.astro'
import GithubLink from '../components/project-links/GithubLink.astro'
import screenshotFullWithAnswer from '../images/nemesis-ai/thumbnail.jpg'
import screenshotMobile from '../images/nemesis-ai/nem-mobile.png'
import nem1 from '../images/nemesis-ai/nem-1.png'
import nem1NoBg from '../images/nemesis-ai/nem-1-no-bg.png'
import nem3 from '../images/nemesis-ai/nem-3.png'
import nemDoc1 from '../images/nemesis-ai/nem-doc-1.png'
import nemDoc2 from '../images/nemesis-ai/nem-doc-2.png'
import ListItem from '../components/ListItem.astro'
---

<ProjectLayout title='Nemesis AI | Laura Cleveland'>
  <div class='[grid-column:2] my-32'>
    <h1 class='mb-16 text-3xl font-bold'>Nemesis AI</h1>
    <p class='mb-10 max-w-xl'>
      Nemesis AI is a bot that I built to answer questions about the rules of
      the board game <a
        href='https://awakenrealms.com/games/nemesis/'
        target='_blank'
        rel='noopener noreferrer'
        class='text-teal-500'>Nemesis</a
      >.
    </p>
    <p class='max-w-xl'>
      The project is a two-part system: the Python scripts that build an index
      from the rules of the game, and the Next app that provides the interface
      and feeds GPT the parts of the index it needs to answer user questions.
    </p>
  </div>

  <div class='[grid-column:2] mb-32'>
    <div class='flex flex-wrap items-center'>
      <div class='grow'>
        <h2 class='mb-8 text-xl font-semibold'>Created with</h2>
        <!-- <ul class=`list-image-[url(/bullet.svg)]`> -->
        <ul>
          <ListItem>Python</ListItem>
          <ListItem>Langchain</ListItem>
          <ListItem>OpenAI API</ListItem>
          <ListItem>React</ListItem>
          <ListItem>Next.js</ListItem>
          <ListItem>Tailwind CSS</ListItem>
        </ul>
      </div>

      <div class='grow shrink-0'>
        <div class='mb-12'>
          <SiteLink url='#' />
        </div>
        <div>
          <GithubLink url='#' />
        </div>
      </div>
    </div>
  </div>

  <div class='[grid-column:2] mb-32'>
    <!-- <Picture
      src={nem1}
      alt='screenshot'
      widths={[1024]}
      aspectRatio='1204:890'
    /> -->
    <Picture
      src={nem1NoBg}
      alt='screenshot'
      widths={[1024]}
      aspectRatio='1126:852'
    />
  </div>

  <div class='[grid-column:2] mb-32'>
    <h2 class='mb-10 text-xl font-bold'>Motivation and purpose</h2>
    <p class='mb-12 max-w-xl'>
      The emergence of new AI tools has been a great opportunity for me to
      explore different corners of programming and find new things to bring into
      web development.
    </p>
    <p class='max-w-xl'>
      I started trying to create a chatbot with a custom knowledge base by using
      GPT Index, now Llama index. It’s a library that does most of the heavy
      lifting in creating your index and querying it. I tried it out with a
      collection of markdown files that were the content of a blog, but there
      was a lot going on behind the scenes that I didn’t understand. I struggled
      through some projects where I did everything from scratch, which helped me
      learn the mechanisms way better. I refined my techniques over a few
      different iterations of question-answering tools, and after building this
      app, I feel more confident going back and learning the more robust tooling
      out there. I better understand the tradeoffs of different ones, and can
      make more informed decisions about which to use for different situations.
    </p>
  </div>

  <div class='[grid-column:2] mb-20'>
    <div class='flex gap-16 lg:gap-24 flex-wrap lg:flex-nowrap lg:flex-row'>
      <div class='sm:basis-2/3 md:basis-3/5 lg:basis-1/2'>
        <Picture
          src={screenshotMobile}
          alt='screenshot'
          widths={[1024]}
          aspectRatio='593:694'
          class='rounded-2xl'
        />
      </div>

      <div class='lg:basis-1/2'>
        <h2 class='mb-4 text-xl font-bold'>Features</h2>
        <p class='mb-6'>
          The format for the system is a single question and answer, as is
          pretty common for tools like those that let you ask questions about a
          podcast series or YouTube channel. This keeps the UI simple and the
          text generation output more predictable, but with GPT it’s possible to
          implement a continuous conversation as well.
        </p>
      </div>
    </div>
  </div>

  <div class='[grid-column:2] mb-20'>
    <p class='max-w-2xl'>
      The app is simple and straightforward, but one extra thing I added for my
      own sanity was a debugging log. During development I quickly found that
      debugging LLM inputs and outputs was a whole different beast than
      debugging I had done for any kind of project I'd done before. The logs
      usually consist of large amounts of text that are hard to parse at a
      glance, and the outputs aren’t deterministic. I added a little modal that
      logs out the data for the current answer. It shows the parts of the source
      material that matched the similarity search, as well as the sections that
      were sent to the API as context for the answer.
    </p>
  </div>

  <div class='[grid-column:2] mb-20'>
    <Picture
      src={nem3}
      alt='screenshot'
      widths={[1024]}
      aspectRatio='1204:694'
      class='rounded-2xl'
    />
  </div>

  <div class='[grid-column:2] mb-32'>
    <h2 class='mb-10 text-xl font-bold'>Challenges</h2>
    <p class='mb-12 max-w-xl'>
      I ran headlong into the ever-persistent problem of LLMs: accuracy. The
      process of question-answering with a custom knowledge base at its most
      basic is pretty straightforward, but the results can be all over the place
      and it can be extremely difficult to get consistent, accurate answers.
    </p>
    <p class='max-w-xl'>
      I had some difficulty with the common method of retrieval where you use a
      similarity search to retrieve chunks from your data, and feed some number
      of those to the completion model. At first I was having trouble retrieving
      the right information from the source material. I needed to have it split
      into large chunks, to include enough surrounding context, but the
      retrieval method works by selecting chunks based on how similar they are
      to the user’s question. Such large chunks meant that lots of them matched
      the question just as well as others that were more relevant. The context
      passed to the LLM was So I did an extra retrieval step; I made the chunks
      small enough that I would get the most relevant ones, and then I would go
      and get the larger sections that those chunks came from. This helped my
      retrieval accuracy and context quality immensely.
    </p>
  </div>

  <div class='[grid-column:2] flex gap-14 mb-32'>
    <div class='basis-2/3'>
      <Picture
        src={nemDoc1}
        alt='screenshot'
        widths={[618]}
        aspectRatio='618:694'
        class='rounded-2xl'
      />
    </div>

    <div class='basis-1/2'>
      <h2 class='mb-10 text-xl font-bold'>Documenting the process</h2>
      <p class='mb-12 max-w-xl'>
        I’ve grown more and more fond of keeping notes as I work on a project,
        and on this project I was the most thorough I’ve been thus far. I like
        to use Obsidian for my notes and have been trying out their Canvas
        feature as a new way of laying everything out. I plan to use these notes
        to put together a blog post on the process.
      </p>
    </div>
  </div>

  <div class='[grid-column:2] mb-40'>
    <Picture
      src={nemDoc2}
      alt='screenshot'
      widths={[1024]}
      aspectRatio='1202:803'
      class='rounded-2xl'
    />
  </div>

  <div class='[grid-column:2] flex justify-center gap-24 mb-40'>
    <div>
      <SiteLink url='#' />
    </div>
    <div>
      <GithubLink url='#' />
    </div>
  </div>
</ProjectLayout>
