---
import { Picture } from '@astrojs/image/components'

import ProjectLayout from '../layouts/ProjectLayout.astro'
import Section from '../components/Section.astro'
import SiteLink from '../components/project-links/SiteLink.astro'
import GithubLink from '../components/project-links/GithubLink.astro'
import ListItem from '../components/ListItem.astro'
import Heading from '../components/Heading.astro'

import screenshotMobile from '../images/nemesis-ai/nem-mobile.png'
import nem1NoBg from '../images/nemesis-ai/nem-1-no-bg.png'
import nem3 from '../images/nemesis-ai/nem-3.png'
import nemDoc1 from '../images/nemesis-ai/nem-doc-1.png'
import nemDoc2 from '../images/nemesis-ai/nem-doc-2.png'
---

<ProjectLayout title='Nemesis AI | Laura Cleveland'>
  <Section>
    <h1 class='mb-16 text-3xl font-bold'>Nemesis AI</h1>
    <p class='mb-10 max-w-xl'>
      Nemesis AI is a bot that I built to answer questions about the rules of
      the board game <a
        href='https://awakenrealms.com/games/nemesis/'
        target='_blank'
        rel='noopener noreferrer'
        class='text-teal-500'>Nemesis</a
      >.
    </p>
    <p class='mb-12 max-w-xl'>
      The project is a two-part system: the Python scripts that build an index
      from the rules of the game, and the Next app that provides the interface
      and feeds GPT the parts of the index it needs to answer user questions.
    </p>

    <Picture
      src={nem1NoBg}
      alt='screenshot'
      widths={[1024]}
      aspectRatio='1126:852'
      class='mb-8'
    />
  </Section>

  <div
    class='[grid-column:2] w-full h-1 bg-gradient-to-br from-[#70cdf5] to-[#6073f7]'
  >
  </div>

  <Section>
    <div class='xs:flex xs:flex-wrap'>
      <div class='xs:grow mb-16 xs:mb-0'>
        <h2 class='mb-6 text-xl font-semibold'>Created with</h2>
        <ul>
          <ListItem>Python</ListItem>
          <ListItem>Langchain</ListItem>
          <ListItem>OpenAI API</ListItem>
          <ListItem>React</ListItem>
          <ListItem>Next.js</ListItem>
          <ListItem>Tailwind CSS</ListItem>
        </ul>
      </div>

      <div
        class='shrink-0 flex justify-center xs:flex-col xs:justify-normal gap-4 xs:gap-8 xs:items-end'
      >
        <SiteLink url='#' />
        <GithubLink url='#' />
      </div>
    </div>
  </Section>

  <Section>
    <Heading>Motivation and purpose</Heading>
    <p class='mb-12 max-w-xl'>
      The emergence of new AI tools has been a great opportunity for me to
      explore different corners of programming and find new things to bring into
      web development.
    </p>
    <p class='max-w-xl'>
      I started trying to create a chatbot with a custom knowledge base by using
      GPT Index, now Llama index. It’s a library that does most of the heavy
      lifting in creating your index and querying it. I tried it out with a
      collection of markdown files that were the content of a blog, but there
      was a lot going on behind the scenes that I didn’t understand. I struggled
      through some projects where I did everything from scratch, which helped me
      learn the mechanisms way better. I refined my techniques over a few
      different iterations of question-answering tools, and after building this
      app, I feel more confident going back and learning the more robust tooling
      out there. I better understand the tradeoffs of different ones, and can
      make more informed decisions about which to use for different situations.
    </p>
  </Section>

  <Section>
    <div class='flex flex-wrap lg:flex-nowrap lg:flex-row'>
      <div class='sm:basis-2/3 md:basis-3/5 lg:basis-2/5'>
        <Picture
          src={screenshotMobile}
          alt='screenshot'
          widths={[1024]}
          aspectRatio='593:694'
          class='mb-10 rounded-2xl'
        />
      </div>

      <div class='lg:basis-1/2 lg:ml-12'>
        <Heading>Features</Heading>
        <p class='mb-6'>
          The format of the system is a single question and answer, as is pretty
          common for tools like those that let you ask questions about a podcast
          series or YouTube channel. This keeps the UI simple and the text
          generation output more predictable, but with GPT it’s possible to
          implement a continuous conversation as well.
        </p>
      </div>
    </div>

    <p class='max-w-2xl'>
      The app is pretty straightforward, but one extra thing I added for my own
      sanity was a debugging log. During development I quickly found that
      debugging LLM inputs and outputs was a whole different beast than
      debugging I had done for any kind of project I'd done before. The logs
      usually consist of large amounts of text that are hard to parse at a
      glance, and the outputs aren’t deterministic. I added a little modal that
      logs out the data for the current answer. It shows the parts of the source
      material that matched the similarity search, as well as the sections that
      were sent to the API as context for the answer.
    </p>

    <Picture
      src={nem3}
      alt='screenshot'
      widths={[1024]}
      aspectRatio='1204:694'
      class='mt-10 rounded-2xl'
    />
  </Section>

  <Section>
    <Heading>Challenges</Heading>
    <p class='mb-12 max-w-xl'>
      I ran headlong into the ever-persistent problem of LLMs: accuracy. The
      process of question-answering with a custom knowledge base at its most
      basic is pretty straightforward, but the results can be all over the place
      and it can be extremely difficult to get consistent, accurate answers.
    </p>
    <p class='max-w-xl'>
      I had some difficulty with the common method of retrieval where you use a
      similarity search to retrieve chunks from your data, and feed some number
      of those to the completion model. At first I was having trouble retrieving
      the right information from the source material. I needed to have it split
      into large chunks, to include enough surrounding context, but the
      retrieval method works by selecting chunks based on how similar they are
      to the user’s question. Such large chunks meant that lots of them matched
      the question just as well as others that were more relevant. The context
      passed to the LLM was So I did an extra retrieval step; I made the chunks
      small enough that I would get the most relevant ones, and then I would go
      and get the larger sections that those chunks came from. This helped my
      retrieval accuracy and context quality immensely.
    </p>
  </Section>

  <Section>
    <div class='flex flex-wrap lg:flex-nowrap lg:flex-row'>
      <div class='sm:basis-2/3 md:basis-3/5 lg:basis-2/5'>
        <Picture
          src={nemDoc1}
          alt='screenshot'
          widths={[618]}
          aspectRatio='618:694'
          class='mb-10 rounded-2xl'
        />
      </div>

      <div class='lg:basis-1/2 lg:ml-12'>
        <Heading>Documenting the process</Heading>
        <p class='mb-12 max-w-xl'>
          I’ve grown more and more fond of keeping notes as I work on a project,
          and on this project I was the most thorough I’ve been thus far. I like
          to use Obsidian for my notes and have been trying out their Canvas
          feature as a new way of laying everything out. I plan to use these
          notes to put together a blog post on the process.
        </p>
      </div>
    </div>

    <Picture
      src={nemDoc2}
      alt='screenshot'
      widths={[1024]}
      aspectRatio='1202:803'
      class='rounded-2xl'
    />
  </Section>

  <Section>
    <div class='flex justify-center gap-4'>
      <SiteLink url='#' />
      <GithubLink url='#' />
    </div>
  </Section>
</ProjectLayout>
